---
title: "OHI 2017 Pressure: Ultraviolet Radiation"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 1
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: "~/github/ohiprep/src/templates/ohi_hdr.html"
pdf_document:
  toc: true
---

# Summary

The Ultraviolet Radiation pressure layer is generated from daily data on Local Noon Erythemal UV Irradiance (mW/m2) derived from satellite observations. 

1. Average the data for each week/year/cell  
1. For each week/year/cell, calculate the mean and sd, so each cell would have ~624 (12*52) values (2004-2016)  
1. Determine which of these were anomalous, defined as greater than the mean plus 1 standard deviation  
1. Sum weekly anomalies for each year/cell (for a total of 52 possible anomalies per year/cell)  
1. Calculate the total number of anomalies in the reference period (in our case, 2004-2009, for a total of 52*5 anomalies per cell)  
1. Calculate the total number of anomalies in the most recent 5 year period (2011-2015)    
1. then for each cell, get the difference between current anomalies and reference anomolies    
1. Rescale the data to be between 0-1 by using the 99.99th quantile as a reference point

# Updates from previous assessment

One additional year of data.

***

# Data Source

**Reference**: The Ultraviolet Radiation pressures layer uses the [Aura OMI GLobal Surface UVB Data Product](http://disc.sci.gsfc.nasa.gov/Aura/data-holdings/OMI/omuvbd_v003.shtml).  
**Native Data Resolution**: 1 degree  
**Values**: Level-3 OMI Surface UV Irradiance and Erythemal Dose- OMUVBd  
**Time Range**: Daily data from 2005 - 2017 (through 7/14/2018, but only full years of data are used)
**Format**: HDF5

***
  
# Methods  

## Setup

```{r setup, message = F, warning = F}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)

## comment out when knitting; set first when doing the data prep!
# setwd("globalprep/prs_uv/v2018") # update to reflect current assessment year!

## rhdf5 package for working with HDF5 files, from bioconductor: http://bioconductor.org/packages/release/bioc/html/rhdf5.html 
# source("http://bioconductor.org/biocLite.R")
# biocLite("rhdf5")

library(raster)
source("../../../src/R/common.R")

library(ncdf4)
library(rgdal)
library(sf) # use simple features rather than sp
library(rhdf5)
library(ggplot2)
library(RColorBrewer)
library(foreach)
library(doParallel)
library(dplyr)
library(readr)
library(stringr)
library(httr) # RCurl?

## update to reflect current assessment year, or whichever year data is being used!!!
data_yr <- "d2018"
raw_data_dir <- file.path(dir_M, "git-annex/globalprep/_raw_data/NASA_OMI_AURA_UV", data_yr)
int_sp_data <- file.path(dir_M, "git-annex/globalprep/prs_uv/v2018/int")

## years of data we are using for this data layer
yrs  <- c(2004:2017)
mths <- str_pad(1:12, 2, "left", pad = "0")
days <- seq(1, 358, 7)

## global ocean raster at 1km for resampling/projecting purposes
ocean <- raster(file.path(dir_M, "model/GL-NCEAS-Halpern2008/tmp/ocean.tif"))
ocean_shp <- st_read(file.path(dir_M, "git-annex/globalprep/spatial/d2014/data"), layer = "regions_gcs")
# ocean_shp <- readOGR(file.path(dir_M, "git-annex/globalprep/spatial/d2014/data"), layer = "regions_gcs", verbose=F)
land <- ocean_shp %>% filter(rgn_typ %in% c("land", "land-disputed", "land-noeez"))

## set mollweide projection CRS
mollCRS <- crs("+proj=moll +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs")
 
## define colors in spectral format for plotting rasters -- rainbow color scheme  
cols <- rev(colorRampPalette(brewer.pal(9, "Spectral"))(255))
```


## Downloading the HDF (.he5) files 

Data files can be found in the [GES DISC EARTHDATA Data Archive](https://disc.gsfc.nasa.gov/datasets/OMUVBd_V003/summary). An EarthData account will be required, and will need to be linked to the GES DISC data archive, see [instructions here](https://disc.gsfc.nasa.gov/earthdata-login). Download the links list, specifying  HDF-EOS5 as the output file format (default). Note this list is __valid for only 2 days__ so a new one must be generated if data is to be downloaded after that time frame. Once the links list _.txt_ file has been downloaded from the portal and saved in the 'raw_data_dir' as defined above, the code below will use the list and earthdata portal login info (username and password) to download the raw HDF files into the raw data directory.

```{r download the data}
## read in file list .text, downloaded from earthdata & saved in destination folder (same as raw_data_dir)
file_list_raw <- read_delim(file.path(raw_data_dir, "file_list.txt"), delim = "\n", col_names = FALSE)
file_list <- file_list_raw %>% 
  mutate(url_str = as.character(X1)) %>% 
  mutate(check_netcdf = str_match(url_str, pattern = "http.*OMI-Aura_L3-OMUVBd.*nc")) %>%
  filter(!is.na(check_netcdf)) %>% 
  select(url_str)
  
## need  username and password, define in console when prompted (or read from secure file), don't save here!!
usrname <- readline("Type earthdata username:")
pass <- readline("Type earthdata password:")

## set up timekeeping for data download
t0 = Sys.time()
n.pym = length(file_list$url_str)
i.pym = 0

## download the data
for(i in 1:3){
  url = as.character(file_list[i,])
  name_raw_file = substr(url, 88, 144)
  
  x = httr::GET(url, authenticate(usrname, pass, type = "basic"), verbose(info = TRUE, ssl = TRUE))
  bin = content(x, "raw")
  writeBin(bin, file.path(raw_data_dir, "data", name_raw_file)) # gnutls_handshake() failed: Handshake failed
  
  i.pym <- i.pym + 1
  min.done <- as.numeric(difftime(Sys.time(), t0, units="mins"))
  min.togo <- (n.pym - i.pym) * min.done/i.pym
  print(sprintf("Retrieving %s of %s. Minutes done=%0.1f, to go=%0.1f",
                i.pym, n.pym, min.done, min.togo)) # approx time remaining for data download
}
```


## Create rasters from HDF files

Calculate weekly means and standard deviations across all years:

```{r calc weekly means and st devs, eval=F}

## list all files from raw data folder
files = list.files(file.path(raw_data_dir, "data"), pattern="*.he5.nc$", full.names=T)

## function to turn HDF file into a raster 
he_to_ras <- function(x){
  
    ncin <- nc_open(x)
    
    h <- h5read(x, attribute)
    r <- raster(h, xmn = -90, ymn = -180, xmx = 90, ymx = 180, 
                crs = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") %>%
            t() %>%
            flip(direction='y')
    
    r[r<0] <- NA # where r = 0, set to NA
    
    return(r)
}

registerDoParallel(8)

## for every week in each year in the time series, calculate the weekly mean and standard deviation.
foreach (yr = yrs) %dopar%{
  
  attribute = ifelse(yr == 2016,"/HDFEOS/GRIDS/OMI UVB Product/Data Fields/ErythemalDailyDose",
                     "/HDFEOS/GRIDS/OMI UVB PRODUCT/Data Fields/ErythemalDailyDose")
  
  l <- files[substr(files[], 96, 99)==yr]
  
  if(yr == 2016){
    
    days <-  c(seq(1,148,7),   # 2 weeks in June are missing. need to account for that when assigning days
               seq(152,351,7))
    
  } else { 
    days <- days
  }
  
  ## now grab weeks
  if(yr == 2016){
    weeks <- c(1:22,25:52) # 2 weeks in June are missing, need to remove these here
  } else {
    weeks <- length(days)
  }
  
  for (j in 1:length(weeks)){
    
    week = weeks[j]
    if(yr == 2016 & j == 22){
      dys <- l[days[j]:(days[j]+3)]
    } else {
      dys   <- l[days[j]:(days[j]+7)] # get all files from the year for these 7 days
    }
    
    rasters   <- lapply(dys[!is.na(dys)], he_to_ras)  # rasterize all of these .nc files. This function will assign NA to the empty ones
    
    uv_week   <- stack(rasters)
    
    ## calculate the mean
    week_mean    <-  calc(uv_week, fun=function(x){mean(x, na.rm = T)},
                          filename = paste0(int_sp_data, "/weekly_means/weekly_means_", yr, "_", week, ".tif"),
                          overwrite = TRUE)
    
    
    week_sd      <-  calc(uv_week, fun = function(x){sd(x, na.rm = T)},
                          filename = paste0(int_sp_data, "weekly_sd/weekly_sd_", yr, "_", week, ".tif"),
                          overwrite = TRUE)
    
    week_mean_sd <-  overlay(week_mean, week_sd, fun = function(x,y){x+y},
                             filename = paste0(int_sp_data, "weekly_mean_sd/weekly_mean_sd_", yr, "_", week, ".tif"),
                             overwrite = TRUE)
  }
}

## get weekly climatologies across all years in the time series
names_weekly <- list.files(file.path(dir_M,'git-annex/globalprep/prs_uv/v2017/int/weekly_means'),full.names=T)

foreach(i = c(1:52)) %dopar% {
  
  ## get all rasters for week i
    if(i %in% c(1:9)){ #did this for the first 9 weeks for filepath naming only
       w = names_weekly[(substr(names_weekly,87,91)==paste0(i,'.tif'))]%>%stack()
    }else{
      w  = names_weekly[(substr(names_weekly,87,88)==i)]%>%stack()
    }
  
  ## mean
  m  = calc(w,fun=function(x){mean(x,na.rm=T)},filename=paste0(file.path(dir_M),
           '/git-annex/globalprep/prs_uv/v2017/int/weekly_climatologies/mean_week_',i,'.tif'),overwrite=T)
  
  ## sd
  sd = calc(w,fun=function(x){sd(x,na.rm=T)},progress='text',filename=paste0(file.path(dir_M),
           '/git-annex/globalprep/prs_uv/v2017/int/weekly_climatologies/sd_week_',i,'.tif'),overwrite=T)
  
  #mean plus sd
  m_sd = overlay(m,sd,fun=function(x,y){x+y},filename = paste0(file.path(dir_M),
           '/git-annex/globalprep/prs_uv/v2017/int/weekly_climatologies/mean_sd_week_',i,'.tif'),overwrite=T)
  
}
```


## Compare to climatology

Compare each week in each year to the climatology for that week. The climatology is equal to the mean plus one standard deviation

```{r compare week to climatology, eval=F}

## second loop to calculate annual positive anomalies
foreach (i = c(2005:2016)) %dopar%{
    
  if(i != 2016){
    s = stack()

    for (j in 1:52){

      w_mean = raster(paste0(file.path(dir_M),'/git-annex/globalprep/prs_uv/v2017/int/weekly_means/weekly_means_',i,'_',j,'.tif')) # mean UV for week i
      w_anom = raster(paste0(file.path(dir_M),'/git-annex/globalprep/prs_uv/v2017/int/weekly_climatologies/mean_sd_week_',j,'.tif')) # get the week climatology
      count = overlay(w_mean,w_anom,fun=function(x,y){ifelse(x>y,1,0)},progress='text') # compare to average anomaly for that week 
      s = stack(s,count)
    }
      
  } else {
    
    ## 2016 is missing 2 weeks in june; this is a hacky way of fixing
    s = stack()

    for (j in c(1:22,25:52)){

      w_mean = raster(paste0(file.path(dir_M),'/git-annex/globalprep/prs_uv/v2017/int/weekly_means/weekly_means_',i,'_',j,'.tif')) # mean UV for week i
      w_anom = raster(paste0(file.path(dir_M),'/git-annex/globalprep/prs_uv/v2017/int/weekly_climatologies/mean_sd_week_',j,'.tif')) # get the week climatology
      count = overlay(w_mean,w_anom,fun=function(x,y){ifelse(x>y,1,0)},progress='text') # compare to average anomaly for that week 
      s = stack(s,count)
    }  
  
  year = calc(s,fun=function(x){sum(x,na.rm=T)},filename=paste0('int/annual_pos_anomalies_',i,'.tif'),overwrite=T)
  }
}

```


## Calculate differences

Calculate the difference in total number of anomalies over a 5 year period compared to the first 5 years (2005-2009)

```{r ref_period, eval=F}

l   <- list.files('int', full.names=TRUE, pattern = "anomalies")

#reference period (2005-2009)
ref <- list.files('int', pattern = "anomalies", full.names=T)[1:5] %>%
        stack() %>%
        calc(., fun=function(x){sum(x, na.rm = TRUE)})

plot(ref,col=cols,axes=F,main = "Total Number of anomalies 2005-2009")
plot(land,add=T)

```

```{r calc_differences, eval=F}

registerDoParallel(8)

foreach(i = c(2005:2012)) %dopar% { # i=2005

  yrs <- c(i:(i+4))

  s   <- stack(l[str_sub(l, -8, -5) %in% yrs]) %>%
        sum(., na.rm=T)

  diff = s - ref #calculate difference between total number of anomalies in recent and historical (2005-2009) time periods

  projection(diff) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
  
  writeRaster(diff, paste0('int/annual_diff_', yrs[1], '_', yrs[5],'.tif'), overwrite = TRUE)
}

```

## Rescale

```{r rescale, eval=F}

out_files <- list.files('int', full.names = TRUE, pattern = 'diff')

## this says 41..my calcs say 36
## I think this has to do with how we calculate the mean and SD using new years of data
## it will change the reference point
read.csv('../../supplementary_information/v2016/reference_points_pressures.csv', stringsAsFactors=F)%>%
             filter(pressure == "Ultraviolet Radiation Anomalies")%>%
             dplyr::select(ref_point)%>%
             as.numeric(.$ref_point)

registerDoParallel(5)

foreach(i = c(2005:2012)) %dopar% { # i=2006
  
  max_yr <- i+4

  out_rescale = out_files[str_sub(out_files, -13, -10) == i] %>%
                 raster() %>%
                  projectRaster(crs = mollCRS, progress='text', over=TRUE, method = 'ngb') %>%
                  resample(ocean, method='ngb') %>%
                  mask(ocean,  filename= file.path(dir_M,      
                      sprintf('git-annex/globalprep/prs_uv/v2017/int/uv_%s_%s-2005_2009_mol_1km.tif', i, max_yr)), overwrite=TRUE)
  
  }

ref_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int/"),
                        full.names = TRUE, pattern = 'mol_1km')

ref_files <- grep("2006_2010|2007_2011|2008_2012|2009_2013|2010_2014|2011_2015", ref_files, value=TRUE)

#get data across all years

for(file in ref_files){ # file <- ref_files[1]
  
  print(file)
  
  the_name <- basename(file)
  the_name <- gsub(".tif", "", the_name)

  m <- raster(file)%>%
    getValues()
  
  saveRDS(na.omit(m), file.path(dir_M, sprintf("git-annex/globalprep/prs_uv/v2017/int/%s_vals.rds", the_name)))
    
}


### put all the data together (otherwise it keeps crashing computer)

val_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int"), pattern = "rds", full = TRUE)

vals <- c()

for(file in val_files){ # file <- ref_files[1]
  
  print(file)
  
  m <- readRDS(file)
  
  vals <- c(vals, m)
  
}

resc_num  <- quantile(vals, prob=0.9999, na.rm=TRUE)  

resc_num <- 36

ref_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int/"),
                        full.names = TRUE, pattern = 'mol_1km.tif')

for(file in ref_files){ # file <- ref_files[1]
  
  print(file)
  
  the_name <- basename(file)
  the_name <- gsub(".tif", "", the_name)
  
  m <- raster(file)%>%
        calc(fun=function(x){ifelse(x>0, 
                             ifelse(x>resc_num, 1, x/resc_num), 
                             0)}, 
                      filename= file.path(dir_M,      
                      sprintf('git-annex/globalprep/prs_uv/v2017/output/%s_rescaled.tif', the_name)), overwrite=TRUE)

}


```

***

# Results
 
```{r output}

out <- raster(file.path(dir_M,'git-annex/globalprep/prs_uv/v2017/output/uv_2012_2016-2005_2009_mol_1km_rescaled.tif'))
plot(out, box=FALSE, col=cols, axes=FALSE, main = 'UV Pressure Layer \n OHI 2017')

```


##Extract data for each region

```{r extract_region_results}

# raster/zonal data
zones <- raster(file.path(dir_M, "git-annex/globalprep/spatial/v2017/regions_eez_with_fao_ant.tif"))
rgn_data <- read.csv("../../../../ohi-global/eez/spatial/rgns_list.csv") %>%
  filter(rgn_id <=250)

# get raster data:
rasts <- list.files(file.path(dir_M,'git-annex/globalprep/prs_uv/v2017/output'), full=TRUE, pattern =  "rescaled")

pressure_stack <- stack(rasts)

```


```{r gif}

library(animation) 
saveGIF({
  for(i in 1:nlayers(pressure_stack)){
      # don't forget to fix the zlimits
      plot(pressure_stack[[i]], zlim = c(0,1),axes=F, col=cols,
           main=names(pressure_stack[[i]]))
      
  }
}, movie.name = 'uv.gif')

```


```{r,eval=F}
# extract data for each region:
regions_stats <- zonal(pressure_stack,  zones, fun="mean", na.rm=TRUE, progress="text")
regions_stats2 <- data.frame(regions_stats)
write.csv(regions_stats2, "int/UV_mean_rgn.csv", row.names = FALSE)
setdiff(regions_stats2$zone, rgn_data$rgn_id) # high seas and Antarctica
setdiff(rgn_data$rgn_id, regions_stats2$zone) # Antarctica is 213

data <- merge(rgn_data, regions_stats2, all.y = TRUE, by.x="rgn_id", by.y="zone") %>%
  tidyr::gather("year", "pressure_score", starts_with("uv")) %>%
  filter(rgn_id <= 250) # filter out non OHI global regions

uv_data <- data %>%
  mutate(year = substring(year, 9, 12)) %>%
  mutate(year = as.numeric(year)) %>%
  dplyr::select(rgn_id, rgn_name, year, pressure_score)


### visualize data using googleVis plot
library(googleVis)
plotData <- uv_data %>%
  dplyr::select(rgn_name, year, pressure_score)

Motion=gvisMotionChart(plotData, 
                       idvar="rgn_name", 
                       timevar="year")
plot(Motion)

print(Motion, file='uv.html')


## save data layer
uv_data <- uv_data %>%
  dplyr::select(rgn_id, year, pressure_score)
write.csv(uv_data, "output/uv.csv", row.names = FALSE)
## quick compare

uv_data <- read.csv("output/uv.csv")
old <- read.csv("../v2016/output/uv_2016.csv") %>%
  mutate(year = 2015) %>%
  select(rgn_id, year, old_pressure = pressure_score) %>%
  left_join(uv_data, by = c("year", "rgn_id"))
plot(old$pressure_score, old$old_pressure)
abline(0,1, col="red")

old <- read.csv("../v2016/output/uv_2016.csv") %>%
  select(rgn_id, old_pressure = pressure_score) %>%
  left_join(read.csv("output/uv_2016.csv"), by = c("rgn_id"))
plot(old$pressure_score, old$old_pressure)
abline(0,1, col="red")

```


***

# Citation information  

Niilo Kalakoski, Panu Lahtinen, Jari Hovila (May 2016) OMI/Aura Surface UVB Irradiance and Erythemal Dose Daily L3 Global Gridded 1.0 degree x 1.0 degree V3, NASA Goddard Space Flight Center.