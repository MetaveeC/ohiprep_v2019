---
title: "OHI 2017 Pressure: Ultraviolet Radiation"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 1
    toc_float: yes
    number_sections: false
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: "~/github/ohiprep/src/templates/ohi_hdr.html"
pdf_document:
  toc: true
---

# Summary

The Ultraviolet Radiation pressure layer is generated from daily data on Local Noon Erythemal UV Irradiance (mW/m2) derived from satellite observations. 

1. Average the data for each week/year/cell  
1. For each week/year/cell, calculate the mean and sd, so each cell would have ~624 (12*52) values (2004-2016)  
1. Determine which of these were anomalous, defined as greater than the mean plus 1 standard deviation  
1. Sum weekly anomalies for each year/cell (for a total of 52 possible anomalies per year/cell)  
1. Calculate the total number of anomalies in the reference period (in our case, 2004-2009, for a total of 52*5 anomalies per cell)  
1. Calculate the total number of anomalies in the most recent 5 year period (2011-2015)    
1. then for each cell, get the difference between current anomalies and reference anomolies    
1. Rescale the data to be between 0-1 by using the 99.99th quantile as a reference point

# Updates from previous assessment

One additional year of data.

***

# Data Source

**Reference**: The Ultraviolet Radiation pressures layer uses the [Aura OMI GLobal Surface UVB Data Product](http://disc.sci.gsfc.nasa.gov/Aura/data-holdings/OMI/omuvbd_v003.shtml).  
**Native Data Resolution**: 1 degree  
**Values**: Level-3 OMI Surface UV Irradiance and Erythemal Dose- OMUVBd  
**Time Range**: Daily data from 2005 - 2017 (10/1/2004 through 7/14/2018, but only full years of data are used)
**Format**: HDF5

***
  
# Methods  

## Setup

```{r setup, message = F, warning = F}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)

## comment out when knitting; set first when doing the data prep!
# setwd("~/github/ohiprep_v2018/globalprep/prs_uv/v2018") # update to reflect current assessment year!

## rhdf5 package for working with HDF5 files, from bioconductor: http://bioconductor.org/packages/release/bioc/html/rhdf5.html 
# source("http://bioconductor.org/biocLite.R")
# biocLite("rhdf5")

library(raster)
source("../../../src/R/common.R")

library(ncdf4)
library(rgdal)
library(sf) # use simple features rather than sp
library(rhdf5)
library(ggplot2)
library(RColorBrewer)
library(foreach)
library(doParallel)
library(dplyr)
library(readr)
library(stringr)
library(httr)
library(lubridate)

## update these 3 to reflect current assessment year, or whichever year data is being used!!!
data_yr <- "d2018"
raw_data_dir <- file.path(dir_M, "git-annex/globalprep/_raw_data/NASA_OMI_AURA_UV", data_yr)
int_sp_data <- file.path(dir_M, "git-annex/globalprep/prs_uv/v2018/int")


## years of data we are using for this data layer
yrs  <- c(2005:2017)
mths <- str_pad(1:12, 2, "left", pad = "0")
days_full <- seq(1, 358, 7)

## global ocean raster at 1km for resampling/projecting purposes
ocean <- raster(file.path(dir_M, "model/GL-NCEAS-Halpern2008/tmp/ocean.tif"))
ocean_shp <- st_read(file.path(dir_M, "git-annex/globalprep/spatial/d2014/data"), layer = "regions_gcs")
# ocean_shp <- readOGR(file.path(dir_M, "git-annex/globalprep/spatial/d2014/data"), layer = "regions_gcs", verbose=F)
land <- ocean_shp %>% filter(rgn_typ %in% c("land", "land-disputed", "land-noeez")) %>% st_geometry()

## set mollweide projection CRS
mollCRS <- crs("+proj=moll +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs")
 
## define colors in spectral format for plotting rasters -- rainbow color scheme  
cols <- rev(colorRampPalette(brewer.pal(9, "Spectral"))(255))
```


## Downloading the NetCDF (.he5.nc) Files 


Data files can be found in the [GES DISC EARTHDATA Data Archive](https://disc.gsfc.nasa.gov/datasets/OMUVBd_V003/summary). An EarthData account will be required, and will need to be linked to the GES DISC data archive, see [instructions here](https://disc.gsfc.nasa.gov/earthdata-login). Download a links list for the variable "ErythemalDailyDose", in NetCDF format. Note this list is __valid for only 2 days__ so a new one must be generated if data is to be downloaded after that time frame. Once the links list _.txt_ file has been downloaded, the code below will use the list and earthdata login info (username and password) to download the new data files into the raw data directory. The naming convention of the downloaded files: 'OMI-Aura' refers to the instrument, 'L3' means it is a level 3 data product, 'OMUVBd' is the measurement, the first date is when the data was recorded, the second date and time corresponds to modification/upload of the data.



```{r earthdata login info, eval = F}
## need  username and password, define in console when prompted (or read from secure file), don't save here!!
usrname <- readline("Type earthdata username:")
pass <- readline("Type earthdata password:")
```


```{r download the data, eval = F}
## this doesn't run on Mazu for some reason; use 'uv_download_data.R' run locally on R to download locally, then move data files over to Mazu

## read in file list .text, downloaded from earthdata & saved in destination folder (same as raw_data_dir)
file_list_raw <- read_delim(file.path(raw_data_dir, "file_list.txt"), delim = "\n", col_names = FALSE)
file_list <- file_list_raw %>% 
  mutate(url_str = as.character(X1)) %>% 
  mutate(check_netcdf = str_match(url_str, pattern = "http.*OMI-Aura_L3-OMUVBd.*nc")) %>%
  filter(!is.na(check_netcdf)) %>% 
  select(url_str)

## set up timekeeping for data download
t0 = Sys.time()
n.pym = length(file_list$url_str)
i.pym = 0

## download the data
for(i in 1:3){
  url = as.character(file_list[i,])
  name_raw_file = substr(url, 88, 144)
  
  x = httr::GET(url, authenticate(usrname, pass, type = "basic"), verbose(info = TRUE, ssl = TRUE))
  bin = content(x, "raw")
  writeBin(bin, file.path(raw_data_dir, "data", name_raw_file)) # gnutls_handshake() failed: Handshake failed
  
  i.pym <- i.pym + 1
  min.done <- as.numeric(difftime(Sys.time(), t0, units="mins"))
  min.togo <- (n.pym - i.pym) * min.done/i.pym
  print(sprintf("Retrieving %s of %s. Minutes done=%0.1f, to go=%0.1f",
                i.pym, n.pym, min.done, min.togo)) # approx time remaining for data download
}

## tip: after downloading, check all .he5.nc files are about the same size i.e. they all downloaded properly/fully
```


## Create rasters from NetCDF files


```{r list and check raw files, eval = F}
## list and check missing dates in NetCDF data files

## list all files from raw data folder
files <- list.files(file.path(raw_data_dir, "data"), pattern = "OMI-Aura_L3-OMUVBd.*.he5.nc$", full.names = TRUE) # netcdf not hdf
files <- files[substr(files, 96, 99) %in% yrs] # select only files for yrs we want

## check all days are there; should go from Oct 2004 through Dec 31 of last full year of data
files_df <- files %>% 
  data.frame() %>% 
  rename(fullname = ".") %>% 
  mutate(post_modify_date = substr(fullname, 111, 119),
         year = substr(fullname, 96, 99), 
         mo = substr(fullname, 101, 102), 
         dy = substr(fullname, 103, 104)) %>%
  mutate(date = paste(year, mo, dy, sep = "-"),
         wk_of_yr = lubridate::week(as_date(date))) %>% 
  group_by(year, wk_of_yr) %>% 
  mutate(nday_in_wk = n()) %>% 
  ungroup() # View(files_df)

check_ndays <- files_df %>%
  group_by(year, mo) %>% # group_by(year) %>% 
  summarize(ndays = n()) # View(check_ndays)
```



### Caluculate Weekly Means and Standard Deviations

Calculate weekly means and standard deviations across all years:

```{r calc weekly means and st devs, eval = F}

## for every week in each year in the time series, calculate the weekly mean and standard deviation
registerDoParallel(3)

# j = 22; yr = 2016 # for testing
## note: 2016 wk 22 has only 4 layers (and 4th is all NA) and length(days)=52 because week 23 is all missing

foreach (yr = yrs) %dopar% {
  
  l <- files[substr(files, 96, 99) == yr]
  
  days_df <- files_df %>%
    filter(year == yr) %>%
    select(wk_of_yr, nday_in_wk) %>%
    distinct() %>% # select just distinct weeks with number of data days they contain
    tidyr::complete(wk_of_yr = seq(1:53)) %>% # possible max 53 weeks
    mutate(nday_in_wk = replace(nday_in_wk, is.na(nday_in_wk), 0)) %>% # zeros if no data
    mutate(lag_nday = lag(nday_in_wk),
           lag_nday = replace(lag_nday, is.na(lag_nday), 1),
           doy = cumsum(lag_nday)) # day-of-year for start of each week of data
  
  
  days <- days_df$doy %>% unique() # eliminate duplicates
  weeks <- days_df$wk_of_yr[days_df$nday_in_wk != 0]
  
  
  for (j in 1:(length(days)-1)) { # print(days[j]:(days[j+1]-1))
    
    wk_files <- l[days[j]:(days[j+1]-1)]
    
    rasters <- raster(wk_files[1], varname = "ErythemalDailyDose")
    for(i in wk_files[-1]){
      r <- raster(i, varname = "ErythemalDailyDose")
      rasters <- stack(rasters, r)
    }
    uv_week <- rasters
    week = str_pad(weeks[j], 2, "left", pad = "0")
    
    
    week_mean <- calc(uv_week, fun = function(x) {mean(x, na.rm = TRUE)},
                      filename = sprintf("%s/weekly_means/weekly_means_%s_%s.tif", 
                                         int_sp_data, yr, week), overwrite = TRUE)
    
    week_sd <- calc(uv_week, fun = function(x) {sd(x, na.rm = TRUE)},
                    filename = sprintf("%s/weekly_sd/weekly_sd_%s_%s.tif", 
                                       int_sp_data, yr, week), overwrite = TRUE)
    
    week_mean_sd <- overlay(week_mean, week_sd, fun = function(x, y) {x + y},
                            filename = sprintf("%s/weekly_mean_sd/weekly_mean_sd_%s_%s.tif", 
                                               int_sp_data, yr, week), overwrite = TRUE)
    
  }
}

## get weekly climatologies across all years in the time series
names_weekly <- list.files(file.path(int_sp_data, "weekly_means"), full.names = TRUE)
match_weeks <- substr(names_weekly, 87, 92)

## check all weeks expected to be there are there
names_weekly_df <- names_weekly %>% 
  data.frame() %>%
  rename(fullname = ".") %>% 
  mutate(yr = substr(fullname, 82, 85),
         wk = substr(fullname, 87, 88))

tmp <- names_weekly_df %>% 
  select(yr, wk) %>% # View(names_weekly_df)
  group_by(yr) %>% 
  summarize(maxwk = max(wk))


foreach(i = match_weeks) %dopar% {
  w = names_weekly[(substr(names_weekly, 87, 92) == i)] %>% stack()
  
  m   <- calc(w, fun = function(x){mean(x, na.rm = TRUE)}, 
              filename = sprintf("%s/weekly_climatologies/mean_week_%s", int_sp_data, i),
              overwrite = TRUE)
  
  sd  <- calc(w, fun = function(x){sd(x,na.rm = TRUE)}, 
              progress = "text",
              filename = sprintf("%s/weekly_climatologies/sd_week_%s", int_sp_data, i),
              overwrite = TRUE)
  
  m_sd <- overlay(m, sd, fun = function(x, y){x + y},
                  filename = sprintf("%s/weekly_climatologies/mean_sd_week_%s", int_sp_data, i),
                  overwrite = TRUE)
}
```



## Compare to Climatology

Compare each week in each year to the climatology for that week. The climatology is equal to the mean plus one standard deviation.

```{r compare week to climatology, eval = F}

## second loop to calculate annual positive anomalies
foreach (i = yrs) %dopar% {
  
  match_weeks <- names_weekly_df %>% filter(yr == i)
  s = stack()
  
  for(j in match_weeks$wk) {
    w_mean = raster(sprintf("%s/weekly_means/weekly_means_%s_%s.tif", int_sp_data, i, j)) # mean UV for week i
    w_anom = raster(sprintf("%s/weekly_climatologies/mean_sd_week_%s.tif", int_sp_data, j)) # the week climatology
    count = overlay(w_mean, w_anom, fun = function(x, y){ifelse(x > y, 1, 0)}) # compare to average anomaly for that week 
    s = stack(s, count)
  }
  
  year = calc(s, fun = function(x){sum(x, na.rm = TRUE)},
              filename = paste0("int/annual_pos_anomalies_", i, ".tif"), 
              overwrite = TRUE)
}

## visualize anomalies, takes a while
library(animation)

pal <- colorRampPalette(rev(brewer.pal(9, "RdYlBu")))(35)
anomalies <- list.files("int", pattern = "anomalies", full.names = TRUE)

saveGIF({expr = for (i in anomalies){
  plot(raster(i), 
       col = pal, 
       axes = FALSE, box = FALSE, legend = FALSE, 
       main = sprintf("Number UV Anomalies %s", substr(i, 26, 29)))
  plot(land, add = TRUE)}},
  ani.width = 800,
  ani.height = 400,
  movie.name = file.path(int_sp_data, "num_anom.gif"))
```


## Calculate Differences

Calculate the difference in total number of anomalies over a 5 year period compared to the first 5 years (2005-2009)

```{r ref_period, eval = F}

l <- list.files("int", full.names = TRUE, pattern = "anomalies")

## reference period is 2005-2009
ref <- list.files("int", pattern = "anomalies", full.names = TRUE)[1:5] %>%
        stack() %>%
        calc(., fun = function(x){sum(x, na.rm = TRUE)})

plot(ref, col = cols, axes = FALSE, main = "Total Number of Anomalies 2005-2009")
plot(land, add = TRUE)
```

```{r calc_differences, eval = F}

registerDoParallel(5)

foreach(i = 2005:(max(yrs) - 4)) %dopar% {
  
  ## calculate difference between total number of anomalies in recent and historical (2005-2009) time periods
  y <- i:(i + 4)
  s <- stack(l[str_sub(l, -8, -5) %in% y]) %>% sum(., na.rm = TRUE)
  diff <- s - ref
  projection(diff) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
  
  writeRaster(diff, filename = sprintf("int/annual_diff_%s_%s.tif", y[1], y[5]), 
              overwrite = TRUE)
  
}
```


## Rescale

```{r rescale, eval = F}

out_files <- list.files("int", full.names = TRUE, pattern = "diff")

## this says 41..my calcs say 36
## I think this has to do with how we calculate the mean and SD using new years of data
## it will change the reference point
read.csv("../../supplementary_information/v2016/reference_points_pressures.csv", stringsAsFactors = FALSE) %>%
             filter(pressure == "Ultraviolet Radiation Anomalies") %>%
             dplyr::select(ref_point) %>%
             as.numeric(.$ref_point)

registerDoParallel(5)

foreach(i = c(2005:2012)) %dopar% { # i=2006
  
  max_yr <- i+4

  out_rescale = out_files[str_sub(out_files, -13, -10) == i] %>%
                 raster() %>%
                  projectRaster(crs = mollCRS, progress='text', over=TRUE, method = 'ngb') %>%
                  resample(ocean, method='ngb') %>%
                  mask(ocean,  filename= file.path(dir_M,      
                      sprintf('git-annex/globalprep/prs_uv/v2017/int/uv_%s_%s-2005_2009_mol_1km.tif', i, max_yr)), overwrite=TRUE)
  
  }

ref_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int/"),
                        full.names = TRUE, pattern = 'mol_1km')

ref_files <- grep("2006_2010|2007_2011|2008_2012|2009_2013|2010_2014|2011_2015", ref_files, value=TRUE)

#get data across all years

for(file in ref_files){ # file <- ref_files[1]
  
  print(file)
  
  the_name <- basename(file)
  the_name <- gsub(".tif", "", the_name)

  m <- raster(file)%>%
    getValues()
  
  saveRDS(na.omit(m), file.path(dir_M, sprintf("git-annex/globalprep/prs_uv/v2017/int/%s_vals.rds", the_name)))
    
}


### put all the data together (otherwise it keeps crashing computer)

val_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int"), pattern = "rds", full = TRUE)

vals <- c()

for(file in val_files){ # file <- ref_files[1]
  
  print(file)
  
  m <- readRDS(file)
  
  vals <- c(vals, m)
  
}

resc_num  <- quantile(vals, prob=0.9999, na.rm=TRUE)  

resc_num <- 36

ref_files <- list.files(file.path(dir_M, "git-annex/globalprep/prs_uv/v2017/int/"),
                        full.names = TRUE, pattern = 'mol_1km.tif')

for(file in ref_files){ # file <- ref_files[1]
  
  print(file)
  
  the_name <- basename(file)
  the_name <- gsub(".tif", "", the_name)
  
  m <- raster(file)%>%
        calc(fun=function(x){ifelse(x>0, 
                             ifelse(x>resc_num, 1, x/resc_num), 
                             0)}, 
                      filename= file.path(dir_M,      
                      sprintf('git-annex/globalprep/prs_uv/v2017/output/%s_rescaled.tif', the_name)), overwrite=TRUE)

}


```

***

# Results
 
```{r output}

out <- raster(file.path(dir_M,'git-annex/globalprep/prs_uv/v2017/output/uv_2012_2016-2005_2009_mol_1km_rescaled.tif'))
plot(out, box=FALSE, col=cols, axes=FALSE, main = 'UV Pressure Layer \n OHI 2017')

```


## Extract data for each region

```{r extract_region_results}

# raster/zonal data
zones <- raster(file.path(dir_M, "git-annex/globalprep/spatial/v2017/regions_eez_with_fao_ant.tif"))
rgn_data <- read.csv("../../../../ohi-global/eez/spatial/rgns_list.csv") %>%
  filter(rgn_id <=250)

# get raster data:
rasts <- list.files(file.path(dir_M,'git-annex/globalprep/prs_uv/v2017/output'), full=TRUE, pattern =  "rescaled")

pressure_stack <- stack(rasts)

```


```{r gif}

library(animation) 
saveGIF({
  for(i in 1:nlayers(pressure_stack)){
      # don't forget to fix the zlimits
      plot(pressure_stack[[i]], zlim = c(0,1),axes=F, col=cols,
           main=names(pressure_stack[[i]]))
      
  }
}, movie.name = 'uv.gif')

```


```{r,eval=F}
# extract data for each region:
regions_stats <- zonal(pressure_stack,  zones, fun="mean", na.rm=TRUE, progress="text")
regions_stats2 <- data.frame(regions_stats)
write.csv(regions_stats2, "int/UV_mean_rgn.csv", row.names = FALSE)
setdiff(regions_stats2$zone, rgn_data$rgn_id) # high seas and Antarctica
setdiff(rgn_data$rgn_id, regions_stats2$zone) # Antarctica is 213

data <- merge(rgn_data, regions_stats2, all.y = TRUE, by.x="rgn_id", by.y="zone") %>%
  tidyr::gather("year", "pressure_score", starts_with("uv")) %>%
  filter(rgn_id <= 250) # filter out non OHI global regions

uv_data <- data %>%
  mutate(year = substring(year, 9, 12)) %>%
  mutate(year = as.numeric(year)) %>%
  dplyr::select(rgn_id, rgn_name, year, pressure_score)


### visualize data using googleVis plot
library(googleVis)
plotData <- uv_data %>%
  dplyr::select(rgn_name, year, pressure_score)

Motion=gvisMotionChart(plotData, 
                       idvar="rgn_name", 
                       timevar="year")
plot(Motion)

print(Motion, file='uv.html')


## save data layer
uv_data <- uv_data %>%
  dplyr::select(rgn_id, year, pressure_score)
write.csv(uv_data, "output/uv.csv", row.names = FALSE)
## quick compare

uv_data <- read.csv("output/uv.csv")
old <- read.csv("../v2016/output/uv_2016.csv") %>%
  mutate(year = 2015) %>%
  select(rgn_id, year, old_pressure = pressure_score) %>%
  left_join(uv_data, by = c("year", "rgn_id"))
plot(old$pressure_score, old$old_pressure)
abline(0,1, col="red")

old <- read.csv("../v2016/output/uv_2016.csv") %>%
  select(rgn_id, old_pressure = pressure_score) %>%
  left_join(read.csv("output/uv_2016.csv"), by = c("rgn_id"))
plot(old$pressure_score, old$old_pressure)
abline(0,1, col="red")

```


***

# Citation information  

Niilo Kalakoski, Panu Lahtinen, Jari Hovila (May 2016) OMI/Aura Surface UVB Irradiance and Erythemal Dose Daily L3 Global Gridded 1.0 degree x 1.0 degree V3, NASA Goddard Space Flight Center.