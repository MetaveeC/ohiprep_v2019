---
title: 'OHI 2019 - Tourism and Recreation '
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    toc: true
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '../../../src/templates/ohi_hdr.html'
  pdf_document:
    toc: true
editor_options: 
  chunk_output_type: console
---


[REFERENCE RMD FILE: https://cdn.rawgit.com/OHI-Science/ohiprep/master/globalprep/tr/v2017/tr_dataprep.html]

###### need to update this link, not working ^^

# Summary
This document describes the steps for obtaining the data used to calculate the tourism and recreation goal for the 2018 global assessment.

The general calculation is:
tr = Ep * Sr * Tw
and
Xtr = tr/90th quantile across regions

* Ep = Proportion of workforce directly employed in tourism
* Sr = (S-1)/5; Sustainability of tourism
* Tw = A penalty applied to regions with travel warnings from the US State Department (or Canada's Government Travel Advise and Advisory)

The following data are used:

* Tourism sustainability: Travel and Tourism Competitiveness Index (TTCI) from World Economic Forum (WEF)
* Proportion of workforce directly employed in tourism: World Travel & Tourism Council ([WTTC](https://www.wttc.org/datagateway))
* Travel warnings: (U.S. State Department)
* Per capita GDP: (World Bank with gaps filled using CIA data), used to gapfill missing values in Tourism sustainability

# Updates from previous assessment
In 2017 we discovered that the WEF-Economics Global Competitiveness Index data used to estimate sustainability is not compatible across years.  New methods are used each year and previous year's of data are not recalculated using the updated methods. Consequently, we use the most recent data for all scenario years.

Travel Warning data form the U.S State department has updated the way data is reported. This year all countries have a travel warning that ranges from level 1 (normal precautions) to level 4 (do not travel). Within each general warning for a country there are regional warnings that are obtain by clicking in each country.
In past assessment we identified subregional warnings and modified the penalty based on this information. This year, because the new way of reporting the data, we will no longer apply this method. We eliminated the "subregion" information/penalties and modified past data to be consistent with our new approach. Future data should be collected without including this information. More information on how to obtain the data in the travel_warning section (scroll down).


The TTCI data is available to download in an excel file directly from the source. File was download on 07/12/2018 and saved in Mazu as a csv. No changes in data.

We were able to update the following data:
* Tourism Sustainability - TTCI form WEF - No updates (in separate .Rmd as of v2019)
* Proportion of jobs in tourism - WTTC data reported until 2017 and unclear if 2018 is actual data or prediction so we used 2017 as the data_year (downloaded: 07/08/2019)
**** should we be using 2018 as the most recent data year? e.g. should this be standard practice? *****
* Travel warnings for 2018 (downloaded: U.S State Department 07/02/2019)



# Initial set-up code

```{r setup, message=FALSE, warning=FALSE}

#library(devtools)
#devtools::install_github("ohi-science/ohicore@dev")
library(ohicore)
library(tidyverse)
library(stringr)
library(WDI)
library(here)

source('https://raw.githubusercontent.com/OHI-Science/ohiprep_v2019/gh-pages/workflow/R/common.R')

## maximum year of wttc data:
year_max <- 2019

source(here("globalprep/tr/v2019/R/tr_fxns.R"))


```


# Ep: Proportion of workforce directly employed in tourism

These data are from the World Travel & Tourism Council (http://www.wttc.org/).  We use "direct" employment data (eee mazu: globalprep/_raw_data/WTTC/d2019/README.md for instructions on obtaining data). The data extend to 2029, but these values are projections.  The actual data goes to 2019.

These data are cleaned and formatted using the R/process_WTTC.R script. Missing values are gapfilled using the UN georegion information.

```{r wttc prop tourism, eval=FALSE}

## describe where the raw data are located:
scenario_yr <- "v2019"
dir_wttc <- file.path(dir_M, 'git-annex/globalprep/_raw_data/WTTC/d2019/raw') # what is this used for?
dir_github <- here("globalprep/tr", scenario_yr)

## processing script that formats the WTTC for OHI, saves the following: intermediate/wttc_empd_rgn
## v2018: source('R/process_WTTC.R', local = TRUE)
source(here("globalprep/tr/v2019/R/process_WTTC.R"))
##### v2019: Duplicates (Guadeloupe/Martinique, PR/VI, China) + see notes on file creation written at the end of the function script! 

## read in the dataset created by above function:
tr_jobs_pct_tour <- read_csv(here('globalprep/tr/v2019/intermediate/wttc_empd_rgn.csv')) %>% 
 dplyr::select(rgn_id, year, jobs_pct)
### change to here() even if its reading the correct file? 

## format data to have complete years/regions and convert percentage of jobs to proportion of jobs
rgn_names <- read_csv('https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/eez/spatial/regions_list.csv') %>%
    dplyr::select(rgn_id)

# create data frame with a row for each combination of region id and year
rgn_names <- expand.grid(rgn_id = rgn_names$rgn_id, 
                             year= min(tr_jobs_pct_tour$year):max(tr_jobs_pct_tour$year)) 
      
tr_data_raw <- rgn_names %>%
  full_join(tr_jobs_pct_tour %>%
                rename(Ep = jobs_pct) %>%
                mutate(Ep = Ep/100) %>%
                mutate(Ep = ifelse(Ep > 1, NA, Ep)),
              by = c('rgn_id', 'year')) %>%
  filter(!rgn_id == 213) %>% 
  filter(!rgn_id == 255) # ditch disputed regions and Antarctica

# v2018: Some regions (rgn_ids 32, 52 and 140 in the 2018 assesment) appear to have an error bteween some years, Ep value > 100%. This line makes this values NA (v2019: Ep all <0.5)
## v2019: Should we be tracking NAs here? >2500 in Ep column
# summary(tr_data_raw)


## gapfill missing data using UN georegion data:
georegions       <- georegions
georegion_labels <- georegion_labels

tr_data_raw <- tr_data_raw %>%
  left_join(georegions, by = 'rgn_id') %>%
  left_join(georegion_labels, by = 'rgn_id') %>%
  select(-r0)
## what do values in r1, r2 columns mean? # of levels for that identifier? 

# Calculate two different gapfill columns using r2 and r1
tr_data_raw_gf <- tr_data_raw %>%
  group_by(year, r2) %>%
  mutate(Ep_pred_r2 = mean(Ep, na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(year, r1) %>%
  mutate(Ep_pred_r1 = mean(Ep, na.rm=TRUE)) %>%
  ungroup()

# first gapfill with r2, if no value available use r1; create column indicating whether value was gapfilled and if so, by what method.
##### note to self: might want to use this same method for natural products gapfilling? should we standardize programatic gapfilling procedure?  
tr_data_raw_gf <- tr_data_raw_gf %>%
  mutate(Ep_all = ifelse(is.na(Ep), Ep_pred_r2, Ep)) %>%
  mutate(Ep_all = ifelse(is.na(Ep_all), Ep_pred_r1, Ep_all)) %>% 
  mutate(gapfilled = ifelse(is.na(Ep) & !is.na(Ep_all), "gapfilled", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & !is.na(Ep_pred_r2), "UN georegion (r2)", NA)) %>%
  mutate(method = ifelse(is.na(Ep) & is.na(Ep_pred_r2) & !is.na(Ep_pred_r1), "UN georegion (r1)", method)) 

# save the gapfill report data
tr_data_gf <- tr_data_raw_gf %>%
  select(rgn_id, year, gapfilled, method) 

write_csv(tr_data_gf, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism_gf.csv"))

tr_data <- tr_data_raw_gf %>%
  select(rgn_id, year, Ep=Ep_all) 

# v2018: write.csv(tr_data, "output/tr_jobs_pct_tourism.csv", row.names=FALSE) -- where was this saving???
write_csv(tr_data, here("globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv"))
## 245 NAs - should we be concerned about this? 

## A quick check to make sure last year's values aren't too crazy different
## (NOTE: the source data has been updated, so there are some changes, but they shouldn't be super different)

old <- read_csv(here('globalprep/tr/v2018/output/tr_jobs_pct_tourism.csv')) %>%
  select(rgn_id, year, ep_old=Ep)

new <- read_csv(here('globalprep/tr/v2019/output/tr_jobs_pct_tourism.csv')) %>%
  left_join(old) %>%
  filter(year==2018) %>%
  arrange(ep_old)

# Visualize data comparison 
library(plotly)
ggplotly(ggplot(new, aes(x = Ep, y = ep_old, labels = rgn_id)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red"))


######## investigate outliers (if applicable)

new_diff <- new %>%
  mutate(difference = ep_old-Ep) %>% 
  filter(!is.na(ep_old)) %>% 
  filter(!is.na(Ep))

outliers <- tr_data_raw %>% 
  filter(rgn_id %in% c(15,116,117,118,119,122,125,127,173,250)) %>% 
  filter(year == 2018) %>% 
  select(rgn_id, rgn_label) %>% 
  left_join(new, by = "rgn_id", "year") %>% 
  mutate(difference = Ep-ep_old) %>% 
  mutate(avg_diff = mean(new_diff$difference))

### v2019: These outliers can be explained by changes in the source data. Compared raw data from v2018 and all of these regions have large changes in the % share of total employment reported for the year 2018. WTTC likely backfills and adjusts their data on a yearly basis.  
```

# Tw: Travel warnings

Primary source of information is from the U.S. State Department  (https://travel.state.gov/content/passports/en/alertswarnings.html)


#### 2018 Assesment
the U.S. State Department updated the way they report the data. This year warnings are reported for every country (not only the ones under a certain risk as it used to be). They now provide a numeric scale describing the level of warning rather than just keywords.  


Previously, we identified subregional warnings and modified the penalty based on this information.  However, given the State Department's new approach, we no longer will apply this method.  We eliminated the "subregion" information/penalties and modified past data to be consistent with our new approach. Future data should be collected without including this information.


#### Notes about getting data:

**For future assessments** It would be worthwhile to see if data can be "scraped" directly from the website into R. This seems possible given the new format of the state department travel warning data.

#### Getting data for 2019 assessment
The following code is used transform the warnings into a multiplier that is used to calculate tourism and recreation scores: 

Step 1: Copy and paste the data for each country: from https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html/. Paste into an excel file, convert to .csv and uploade to raw folder

*Date downloaded*: 1 July 2019

*Date range of warnings*: 18 June 2018 - 1 July 2019

Step 2: Wrangle and clean the new data

```{r, eval=FALSE}
##Reading and wrangling 2019 warning data

warn_raw <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2019_raw.csv'), na.strings = " ") %>% 
  mutate(country = as.character(country)) 

# Count the number of warnings per country so that those with only regional-based warnings are not removed in the next step
warn_count <- warn_raw %>% 
  group_by(country) %>% 
  tally() %>% 
  ungroup()

# Remove text information from level, join with warning counts, and filter duplicates
warn_raw <- warn_raw %>% 
  mutate(level = as.numeric(str_extract(level, '[1,2,3,4]'))) %>% 
  left_join(.,warn_count, by = "country") %>% 
  filter(!(regional %in% 1 & n >1)) %>% # remove regions that have regional warnings in addition to country wide warnings, but retain those where country warning level is based on regional information
  select(assess_year, level, country) %>% 
  rename(year = assess_year)


### v2018: 
#warn_raw <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2019_raw.csv'), na.strings = " ") %>% 
#  mutate(country = as.character(country)) %>% 
#  mutate(level = as.numeric(str_extract(level, '[1,2,3,4]'))) %>% 
#  filter(!(regional %in% 1)) %>% # no longer relevant because we are not adjusting penalty based on subregional information
#  ##### ^ this might not be the best way to do this, as it removes regions that only have one line of data (country warning level is based on regional information)
#  select(assess_year, level, country) %>% 
#  rename(year= assess_year)

 
##Correct regions that are reported together - check to make sure these are necessary and that everything is covered as data change from year to year. 

french_indies <- data.frame(country="French West Indies", 
                            country_new =c("Northern Saint-Martin")) %>%
  left_join(filter(warn_raw, country=="French West Indies")) %>%
  select(country=country_new, year, level)

BES <- data.frame(country="Bonaire, St. Eustatius, and Saba", 
                            country_new =c("Bonaire", "Saba", "Sint Eustatius")) %>%
  left_join(filter(warn_raw, country=="Bonaire, St. Eustatius, and Saba (BES)")) %>%
    select(country=country_new, year, level)
# need to include level here 

line <- data.frame(country="Line Islands (Kiribati)", 
                            country_new =c("Line Group", "Phoenix Group")) %>%
  left_join(filter(warn_raw, country=="Line Islands (Kiribati)")) %>%
    select(country=country_new, year, level)

warn_improved <- filter(warn_raw, country != "French West Indies") %>%
  bind_rows(french_indies) 

warn_improved <- filter(warn_improved, country != "Bonaire, St. Eustatius, and Saba (BES)") %>%
  bind_rows(BES) 

warn_improved <- filter(warn_improved, country != "France *Monaco") %>%
  bind_rows(FandM) 

warn_improved <- filter(warn_improved, country != "Line Islands (Kiribati)") %>%
  bind_rows(line) 


##Correct names for regions not identified by the name_2_region
warn_improved[str_detect(warn_improved$country,"union"), ] #use to check names

warn_improved <- warn_improved %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Israel"), "Israel", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Bonaire"), "Bonaire", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"ROC"), "Republique du Congo", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Kinshasha"), "Democratic Republic of the Congo", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Domincan"), "Dominican Republic", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Honduras"), "Honduras", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Sao Tome"), "Sao Tome and Principe", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"Turks and Caicos"), "Turks and Caicos Islands", country)) %>%
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"United Arab Emirates"), "United Arab Emirates", country)) %>% 
  dplyr::mutate(country = ifelse(stringr::str_detect(country,"U.S. Virgin Islands"), "Puerto Rico and Virgin Islands of the United States", country)) %>% # creates duplicate of PR/VI
    dplyr::mutate(country = ifelse(stringr::str_detect(country,"Puerto Rico"), "Puerto Rico and Virgin Islands of the United States", country))

##NOTE: Puerto Rico and Virgin Islands are reported by the Canadian Goverment Travel Advisory (SEE BELOW befre runnig the script) ??

#v2019: Guadeloupe + Guadeloupe and Martinique are duplicated here; same with PR/VI and Bonaire ; plus there are a couple of NAs. Put in a line here to remove duplicates and NAs? 
# Also, this code isn't 

```

Step 3: Join new data with previous years' assesment data
(adjusting previous data to be consistent with this year's data)

In 2018 we adjusted the data for provious years and saved the complete warning list up to 2018 in the intermediate folder (warning_complete.csv). This file should be combined with the data downloaded in 2019 (if data is reported in the same format)
v2019 note - this data doesn't need to be combined, because the travel advisory website has all of the data up to date that covers this entire range between the assessments. (We should be able to skip this section in the 2019 OHI assessment. )
```{r, eval=FALSE}


## v2019: for this year I think we dont need to use gather as the data are being reported differently; added in str_extract to remove text indicating detail about level

warn_old <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2018_raw.csv')) %>%
  select(year = assess_year, country, regional, level) %>% 
  mutate(level = as.numeric(str_extract(level, "\\-*\\d+\\.*\\d*")))

# some countries will be duplicated here because of regional warnings; removed below 



## v2018: deleted a couple regions that were duplicated (2016 Haiti and 2017 Kenya)
# v2018 import: 
# apply_level = data.frame(warning = c("inform", "risk", "avoid_nonessential", "avoid_all", "gtfo"), level = c(1, 2, 3, 4, 4)) 
# warn_old <- read.csv(here('globalprep/tr/v2019/raw/tr_travelwarning_2018_raw.csv')) %>% 
#select(year = assess_year, country=rgn_name, inform, risk, avoid_nonessential, avoid_all, gtfo, regional) %>%
#  gather("warning", "n", 3:7)  %>%
#  filter(!is.na(n)) %>%
#  select(-n) %>% 
#left_join(apply_level, by="warning") %>%
#  select(-warning) %>% 
#  arrange(desc(year), country) 

## The old data does not always have data for the larger region when there was a "regional" warning.
## Adding that data here:

warn_old_add_rgns <- warn_old %>%
  rowwise() %>%
  mutate(country_year = paste(country, year, sep="_")) %>%
  arrange(year, country) %>%
  group_by(year, country) %>%
  mutate(region_data = sum(is.na(regional)))

#filtering for the countries that only have one regional warning in order to give the country as a whole a level 1
warn_old_add_rgns <- filter(warn_old_add_rgns, region_data==0) %>% 
  mutate(regional = 0) %>%
  mutate(level = 1) %>%
  select(year, country, level)

### v2019: this df doesn't have any data

## exclude subregion data from old data
warn_old_no_subs <- filter(warn_old, !(regional %in% 1)) %>%
  select(year, country, level) %>% 
  mutate(country = as.character(country))


#binding old and new data
warn_complete <- warn_improved %>% #2019 data
  bind_rows(warn_old_no_subs) %>% #old with no regional
  bind_rows(warn_old_add_rgns)  #old with regional warning transformed into a level 1 for the whole country
# check to make sure that all regions are represented for both years
# something still going wrong with Bonaire, St. Eustatius and Saba; 

##Save warn_complete. This file will be the one that needs to be combined with the new warnings in 2019. -- wasn't it already at this point?

write_csv(warn_complete, here("globalprep/tr/v2019/intermediate/warning_complete.csv"))


```

Step 4: Transform the warnings into a multiplier that is used to calculate tourism and recreation scores.

Travel warning  | Multiplier   | Description
--------------- | ------------ | -------------------
Level 1 | 1 (no penalty) | Exercise Normal Precautions: This is the lowest advisory level for safety and security risk. There is some risk in any international travel. 
Level 2 | 1 (no penalty) | Exercise Increased Caution:  Be aware of heightened risks to safety and security. 
Level 3 | 0.25  | Reconsider Travel: Avoid travel due to serious risks to safety and security.
Level 4 | 0 (full penalty, results in zero scores) | Do Not Travel:  This is the highest advisory level due to greater likelihood of life-threatening risks. 

```{r, eval=FALSE}
warn_complete <- read.csv(here("globalprep/tr/v2018/intermediate/warning_complete.csv"))

scores <-  data.frame(level = c(1, 2, 3, 4), multiplier = c(1, 1, 0.25, 0)) 


warn_multiplier <-  warn_complete %>%  
  left_join(scores, by="level") %>% 
  group_by(year, country) %>%
  mutate(warning_count = n()) %>%
  ungroup()

# in general should be no regions with more than one warning count, \
# but Puerto Rico and Virgin Islands are fine because they were reported separately
# these will be averaged
filter(warn_multiplier, warning_count>1)

warn_multiplier <- warn_multiplier %>%
  group_by(year, country) %>%
  summarize(multiplier = mean(multiplier))

#Save file with multiplier
write.csv(warn_multiplier, here("globalprep/tr/v2018/intermediate/warning.csv"), row.names=FALSE)


```


Step 5: Convert names to OHI regions and clean. 

```{r travel warnings, eval=FALSE}
warn <- read.csv(here("globalprep/tr/v2018/intermediate/warning.csv"))

#Add rgn_id
warn_rgn <- name_2_rgn(df_in = warn, 
                       fld_name='country', 
                       flds_unique=c('country','year'))

# Check to see if any regions are duplicated:
sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))
# China has multiple warnings (rgn_id 209)

# Average China warnings
warn_rgn <- warn_rgn %>%
  group_by(rgn_id, rgn_name, year) %>%
  summarize(multiplier = mean(multiplier)) %>%
  ungroup()

sort(table(paste(warn_rgn$year, warn_rgn$rgn_id)))

```


Next year, consider gapfilling missing territorial regions with administrative country data.


Final step is to **compare with previous year's data**.

Many European regions now have a travel warning due to increased terrorism (e.g., United Kingdom, Italy, Spain, Germany), although this doesn't show up in the following figure because previously, these regions had no travel warning (and were thus, NA).

The change in not penalizing subregional warnings tended to reduce the penalty (i.e. increase the multiplier value).
```{r, eval=FALSE}
# The following indicates changes over time as well as changes to the State Department's approach to quantifying risk
tmp <- warn_rgn %>%
  spread(year, multiplier) %>%
  data.frame()

plot(jitter(tmp$X2017), jitter(tmp$X2018))
abline(0,1, col="red")


### compare against last year's data
### these changes will reflect changes due to removing subregional warnings:
old <- read.csv(here("globalprep/tr/v2017/output/tr_travelwarnings.csv")) %>%
  filter(year==2017) %>%
  left_join(tmp, by="rgn_id")


plot(jitter(old$multiplier), jitter(old$X2017))
abline(0,1, col="red")

```

### Save the travel warning data in the output folder

```{r, eval=FALSE}
georegions <- georegion_labels %>%
  select(rgn_id)
  

warn_rgn_spread <- warn_rgn %>%
  spread(year, multiplier) %>%
  full_join(georegions, by=c("rgn_id")) %>%
  data.frame() %>%
  gather(year, multiplier, starts_with("X")) %>%
  mutate(year = gsub("X", "", year)) %>%
  mutate(multiplier = ifelse(is.na(multiplier), 1, multiplier)) %>% #wth a multiplier 1 to all regions with no warnings
  filter(rgn_id <= 250) %>%
  filter(rgn_id != 213) 
  
#should by 220 for each year
table(warn_rgn_spread$year)

warn_rgn_all_rgns <- warn_rgn_spread %>%
  select(rgn_id, year, multiplier) %>%
  arrange(year, rgn_id)

write.csv(warn_rgn_all_rgns, here('globalprep/tr/v2018/output/tr_travelwarnings.csv'), row.names=FALSE)

## Create gapfill file. No gapfill in this case so gapfill = 0

travelwarning_gf <- read.csv(here("globalprep/tr/v2018/output/tr_travelwarnings.csv")) %>% 
  mutate(multiplier = 0) %>% 
  rename(gapfilled = multiplier)

write.csv(travelwarning_gf, here('globalprep/tr/v2018/output/tr_travlewarnings_gf.csv'), row.names = FALSE)

```

